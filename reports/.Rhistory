y = BreastCancer[,11]
#convert the y factors to quantitative variables
y = as.numeric(y)-1
#Extract the predictor variables
X1_raw = BreastCancer[,2:10]
X1_raw
#convert the factors to quantitative variables
X1_raw = apply(X1_raw,2, as.numeric)
#standardise the predictor variables æ ‡å‡†åŒ–é¢„æµ‹å˜é‡?X1 = scale(as.matrix(X1_raw))
#reform data frame  #é‡æ–°ç”Ÿæˆä¸€ä¸ªdata frame
BreastCancer_data = data.frame(y,X1)
head(BreastCancer_data)
#apply the best subset selection approach
#load the package
library(leaps)
#apply the regsubsets function  dataè¦ç”¨é‡æ–°ç”Ÿæˆçš„é‚£ä¸ªyåœ¨å‰çš„æ•°æ®æ¡†æž?BreastCancer_bss = regsubsets(y~.,data =BreastCancer_data, method = "exhaustive", nvmax = 10 )
#Applying the summary function to the returned object gives the â€œbestâ€?models
BreastCancer_bss_summary = summary(BreastCancer_bss)
#the optimal value of K in each case
best_adjr2 = which.max(BreastCancer_bss_summary$adjr2)
best_cp = which.min(BreastCancer_bss_summary$cp)
best_bic = which.min(BreastCancer_bss_summary$bic)
#decide on the â€œbestâ€?mode by producing a graphical summary
#create a plotting device
par(mfrow=c(1,3))
#produce plots and highlight optimal value of k
plot(1:9, BreastCancer_bss_summary$adjr2,xlab = "Number of predictors", ylab ="Adjusted Rsq", type = "b")
points(best_adjr2,BreastCancer_bss_summary$adjr2[best_adjr2],col = "red", pch=16 )
plot(1:9, BreastCancer_bss_summary$cp,xlab = "Number of predictors", ylab ="CP", type = "b")
points(best_cp,BreastCancer_bss_summary$cp[best_cp],col = "red", pch=16 )
plot(1:9, BreastCancer_bss_summary$bic,xlab = "Number of predictors", ylab ="BIC", type = "b")
points(best_bic,BreastCancer_bss_summary$bic[best_bic],col = "red", pch=16 )
#obtain the least squares estimates of the coefficients for M7
coef(BreastCancer_bss,7)
#rebuild the data
head(BreastCancer_data)
BreastCancer_data_rebuild = BreastCancer_data[,-6]
BreastCancer_data_rebuild = BreastCancer_data_rebuild[,-9]
head(BreastCancer_data_rebuild)# ä¸¢æŽ‰äº†Epith.c.size   Bare.nuclei   Bl.cromatin
#now we can fit a logistic regression model using the glm function
lr_fit = glm(y~.,data = BreastCancer_data_rebuild, family="binomial")
#summarize the model fit
summary(lr_fit)   #è¿™æ˜¯ç”Ÿæˆçš„ç¬¬ä¸€ä¸ªlogisticå›žå½’ï¼Œç”¨çš„æœ€ä½³å­é›†é€‰æ‹©æ³?coef(lr_fit)
##one regularized form of logistic regression
#load the package
library(glmnet)
#choose grid of values for the tuning parameter
grid = 10^seq(5,-3,length=100)
#fit a ridge regression model for each value of the tumning parameter
ridge_fit = glmnet(X1,y,alpha=0,standardize=FALSE, lambda = grid)
#choose an appropriate value for tuning parameter
ridge_cv_fit = cv.glmnet(X1,y,alpha=0,standardize=FALSE, lambda = grid)
#display how the cross-validated error varies with Î»
plot(ridge_cv_fit)
#extract the value of Î» corresponding to the minimum
lambda_ridge_min = ridge_cv_fit$lambda.min
lambda_ridge_min
#fit a logstic regression model
ridge_lr_fit = glmnet(as.matrix(BreastCancer_data), y, alpha=0, family="binomial", lambda=lambda_ridge_min )
coef(ridge_lr_fit)
summary(ridge_lr_fit)
test_error_model2
#model 2
#construct a test data
train_set_model2 = sample(c(TRUE,FALSE),nrow(BreastCancer_data),replace=TRUE)
lr_train_model2 = glmnet(BreastCancer_data[train_set_model2,-1], BreastCancer_data[train_set_model2,1], alpha=0, family="binomial", lambda=lambda_ridge_min )
phat_test2 = predict(lr_train_model2,BreastCancer_data[!train_set_model2,-1],s=lambda_ridge_min,family="response")
testdata = apply(BreastCancer_data[!train_set_model2,-1],2,as.numeric)
phat_test2 = predict(lr_train_model2,testdata,s=lambda_ridge_min,family="response")
yhat_test2 = ifelse(phat_test2>0.5,1,0)
#compute the test error for model 2
test_error_model2 = 1 - mean(BreastCancer_data$y[!train_set_model2] == yhat_test2)
test_error_model2
#model 1
#construct a test data
train_set_model1 = sample(c(TRUE,FALSE),nrow(BreastCancer_data_rebuild),replace=TRUE)
lr_train_model1 = glm(y~.,data = BreastCancer_data_rebuild[train_set_model1,], family="binomial")
phat_test1 = predict(lr_train_model1,BreastCancer_data_rebuild[!train_set_model1,],family="response")
yhat_test1 = ifelse(phat_test1>0.5,1,0)
phat_test1
#compute the test error for model 1
test_error_model1 = 1 - mean(BreastCancer_data_rebuild$y[!train_set_model1] == yhat_test1)
test_error_model1
test_error_model2
#compute the test error for model 3
lda_fit_model3$error_rate
knitr::opts_chunk$set(echo = TRUE)
##load mlbench package
#install.packages("mlbench")
library(mlbench)
##load the data
data("BreastCancer")
#1. cleaning the data(remove na rows)
BreastCancer = na.omit(BreastCancer)
install.packages("tinytex")
tinytex::install_tinytex()
#Extract the response variable
y = BreastCancer[,11]
#convert the y factors to quantitative variables
y = as.numeric(y)-1
#Extract the predictor variables
X1_raw = BreastCancer[,2:10]
X1_raw
#convert the factors to quantitative variables
X1_raw = apply(X1_raw,2, as.numeric)
#standardise the predictor variables æ ‡å‡†åŒ–é¢„æµ‹å˜é‡?X1 = scale(as.matrix(X1_raw))
#load the package
library(glmnet)
#choose grid of values for the tuning parameter
grid = 10^seq(5,-3,length=100)
#fit a ridge regression model for each value of the tumning parameter
ridge_fit = glmnet(X1,y,alpha=0,standardize=FALSE, lambda = grid)
#choose an appropriate value for tuning parameter
ridge_cv_fit = cv.glmnet(X1,y,alpha=0,standardize=FALSE, lambda = grid)
#display how the cross-validated error varies with Î»
plot(ridge_cv_fit)
#extract the value of Î» corresponding to the minimum
lambda_ridge_min = ridge_cv_fit$lambda.min
lambda_ridge_min
##load mlbench package
#install.packages("mlbench")
library(mlbench)
##load the data
data("BreastCancer")
#1. cleaning the data(remove na rows)
BreastCancer = na.omit(BreastCancer)
#Extract the response variable
y = BreastCancer[,11]
#convert the y factors to quantitative variables
y = as.numeric(y)-1
#Extract the predictor variables
X1_raw = BreastCancer[,2:10]
#convert the factors to quantitative variables
X1_raw = apply(X1_raw,2, as.numeric)
#standardise the predictor variables
X1 = scale(as.matrix(X1_raw))
#reform data frame
BreastCancer_data = data.frame(y,X1)
#head(BreastCancer_data)
#apply the best subset selection approach
#load the package
library(leaps)
#apply the regsubsets function  data
BreastCancer_bss = regsubsets(y~.,data =BreastCancer_data, method = "exhaustive", nvmax = 10 )
#Applying the summary function to the returned object gives the â€œbestâ€?models
BreastCancer_bss_summary = summary(BreastCancer_bss)
#the optimal value of K in each case
best_adjr2 = which.max(BreastCancer_bss_summary$adjr2)
best_cp = which.min(BreastCancer_bss_summary$cp)
best_bic = which.min(BreastCancer_bss_summary$bic)
#decide on the â€œbestâ€?mode by producing a graphical summary
#create a plotting device
par(mfrow=c(1,3))
#produce plots and highlight optimal value of k
plot(1:9, BreastCancer_bss_summary$adjr2,xlab = "Number of predictors", ylab ="Adjusted Rsq", type = "b")
points(best_adjr2,BreastCancer_bss_summary$adjr2[best_adjr2],col = "red", pch=16 )
plot(1:9, BreastCancer_bss_summary$cp,xlab = "Number of predictors", ylab ="CP", type = "b")
points(best_cp,BreastCancer_bss_summary$cp[best_cp],col = "red", pch=16 )
plot(1:9, BreastCancer_bss_summary$bic,xlab = "Number of predictors", ylab ="BIC", type = "b")
points(best_bic,BreastCancer_bss_summary$bic[best_bic],col = "red", pch=16 )
#coef(BreastCancer_bss,7)
#rebuild the data
#head(BreastCancer_data)
BreastCancer_data_rebuild = BreastCancer_data[,-6]
BreastCancer_data_rebuild = BreastCancer_data_rebuild[,-9]
#head(BreastCancer_data_rebuild)# Epith.c.size   Bare.nuclei   Bl.cromatin
#now we can fit a logistic regression model using the glm function
lr_fit = glm(y~.,data = BreastCancer_data_rebuild, family="binomial")
#summarize the model fit
#summary(lr_fit)
coef(lr_fit)
#load the package
library(glmnet)
#choose grid of values for the tuning parameter
grid = 10^seq(5,-3,length=100)
#fit a ridge regression model for each value of the tuning parameter
ridge_fit = glmnet(X1,y,alpha=0,standardize=FALSE, lambda = grid)
#choose an appropriate value for tuning parameter
ridge_cv_fit = cv.glmnet(X1,y,alpha=0,standardize=FALSE, lambda = grid)
#display how the cross-validated error varies with Î»
plot(ridge_cv_fit)
#extract the value of Î» corresponding to the minimum
lambda_ridge_min = ridge_cv_fit$lambda.min
lambda_ridge_min
#fit a logstic regression model
ridge_lr_fit = glmnet(as.matrix(BreastCancer_data), y, alpha=0, family="binomial", lambda=lambda_ridge_min )
coef(ridge_lr_fit)
#ummary(ridge_lr_fit)
#model 1
#construct a test data
train_set_model1 = sample(c(TRUE,FALSE),nrow(BreastCancer_data_rebuild),replace=TRUE)
lr_train_model1 = glm(y~.,data = BreastCancer_data_rebuild[train_set_model1,], family="binomial")
phat_test1 = predict(lr_train_model1,BreastCancer_data_rebuild[!train_set_model1,],family="response")
yhat_test1 = ifelse(phat_test1>0.5,1,0)
#phat_test1
#compute the test error for model 1
test_error_model1 = 1 - mean(BreastCancer_data_rebuild$y[!train_set_model1] == yhat_test1)
test_error_model1
#model 2
#construct a test data
train_set_model2 = sample(c(TRUE,FALSE),nrow(BreastCancer_data),replace=TRUE)
lr_train_model2 = glmnet(BreastCancer_data[train_set_model2,-1], BreastCancer_data[train_set_model2,1], alpha=0, family="binomial", lambda=lambda_ridge_min )
testdata = apply(BreastCancer_data[!train_set_model2,-1],2,as.numeric)
phat_test2 = predict(lr_train_model2,testdata,s=lambda_ridge_min,family="response")
yhat_test2 = ifelse(phat_test2>0.5,1,0)
#compute the test error for model 2
test_error_model2 = 1 - mean(BreastCancer_data$y[!train_set_model2] == yhat_test2)
test_error_model2
#model 3
training_indices = which(train_set_model2)
validation_indices = which(!train_set_model2)
lda_fit_model3 = linDA(variables = BreastCancer_data[,-1],group = BreastCancer_data$y,validation = "learntest",
learn =training_indices,test=validation_indices )
#model 1
#construct a test data
train_set_model1 = sample(c(TRUE,FALSE),nrow(BreastCancer_data_rebuild),replace=TRUE)
lr_train_model1 = glm(y~.,data = BreastCancer_data_rebuild[train_set_model1,], family="binomial")
phat_test1 = predict(lr_train_model1,BreastCancer_data_rebuild[!train_set_model1,],family="response")
yhat_test1 = ifelse(phat_test1>0.5,1,0)
#phat_test1
#compute the test error for model 1
test_error_model1 = 1 - mean(BreastCancer_data_rebuild$y[!train_set_model1] == yhat_test1)
test_error_model1
#model 2
#construct a test data
train_set_model2 = sample(c(TRUE,FALSE),nrow(BreastCancer_data),replace=TRUE)
lr_train_model2 = glmnet(BreastCancer_data[train_set_model2,-1], BreastCancer_data[train_set_model2,1], alpha=0, family="binomial", lambda=lambda_ridge_min )
testdata = apply(BreastCancer_data[!train_set_model2,-1],2,as.numeric)
phat_test2 = predict(lr_train_model2,testdata,s=lambda_ridge_min,family="response")
yhat_test2 = ifelse(phat_test2>0.5,1,0)
#compute the test error for model 2
test_error_model2 = 1 - mean(BreastCancer_data$y[!train_set_model2] == yhat_test2)
test_error_model2
#model 3
library(nclSLR)
training_indices = which(train_set_model2)
validation_indices = which(!train_set_model2)
lda_fit_model3 = linDA(variables = BreastCancer_data[,-1],group = BreastCancer_data$y,validation = "learntest",
learn =training_indices,test=validation_indices )
#compute the test error for model 3
lda_fit_model3$error_rate
#model 1
#construct a test data
train_set_model1 = sample(c(TRUE,FALSE),nrow(BreastCancer_data_rebuild),replace=TRUE)
lr_train_model1 = glm(y~.,data = BreastCancer_data_rebuild[train_set_model1,], family="binomial")
phat_test1 = predict(lr_train_model1,BreastCancer_data_rebuild[!train_set_model1,],family="response")
yhat_test1 = ifelse(phat_test1>0.5,1,0)
#phat_test1
#compute the test error for model 1
test_error_model1 = 1 - mean(BreastCancer_data_rebuild$y[!train_set_model1] == yhat_test1)
test_error_model1
#model 2
#construct a test data
train_set_model2 = sample(c(TRUE,FALSE),nrow(BreastCancer_data),replace=TRUE)
lr_train_model2 = glmnet(BreastCancer_data[train_set_model2,-1], BreastCancer_data[train_set_model2,1], alpha=0, family="binomial", lambda=lambda_ridge_min )
testdata = apply(BreastCancer_data[!train_set_model2,-1],2,as.numeric)
phat_test2 = predict(lr_train_model2,testdata,s=lambda_ridge_min,family="response")
yhat_test2 = ifelse(phat_test2>0.5,1,0)
#compute the test error for model 2
test_error_model2 = 1 - mean(BreastCancer_data$y[!train_set_model2] == yhat_test2)
test_error_model2
#model 3
library(nclSLR)
training_indices = which(train_set_model2)
validation_indices = which(!train_set_model2)
lda_fit_model3 = linDA(variables = BreastCancer_data[,-1],group = BreastCancer_data$y,validation = "learntest",
learn =training_indices,test=validation_indices )
#compute the test error for model 3
lda_fit_model3$error_rate
#model 1
test_error_model1_list = c()
for (i in 1:100) {
#construct a test data
train_set_model1 = sample(c(TRUE,FALSE),nrow(BreastCancer_data_rebuild),replace=TRUE)
lr_train_model1 = glm(y~.,data = BreastCancer_data_rebuild[train_set_model1,], family="binomial")
phat_test1 = predict(lr_train_model1,BreastCancer_data_rebuild[!train_set_model1,],family="response")
yhat_test1 = ifelse(phat_test1>0.5,1,0)
#phat_test1
#compute the test error for model 1
test_error_model1 = 1 - mean(BreastCancer_data_rebuild$y[!train_set_model1] == yhat_test1)
test_error_model1_list = c(test_error_model1_list,test_error_model1)
}
test_error_mean1 = mean(test_error_model1_list)
#model 2
#construct a test data
train_set_model2 = sample(c(TRUE,FALSE),nrow(BreastCancer_data),replace=TRUE)
lr_train_model2 = glmnet(BreastCancer_data[train_set_model2,-1], BreastCancer_data[train_set_model2,1], alpha=0, family="binomial", lambda=lambda_ridge_min )
testdata = apply(BreastCancer_data[!train_set_model2,-1],2,as.numeric)
phat_test2 = predict(lr_train_model2,testdata,s=lambda_ridge_min,family="response")
yhat_test2 = ifelse(phat_test2>0.5,1,0)
#compute the test error for model 2
test_error_model2 = 1 - mean(BreastCancer_data$y[!train_set_model2] == yhat_test2)
test_error_model2
#model 3
library(nclSLR)
training_indices = which(train_set_model2)
validation_indices = which(!train_set_model2)
lda_fit_model3 = linDA(variables = BreastCancer_data[,-1],group = BreastCancer_data$y,validation = "learntest",
learn =training_indices,test=validation_indices )
#compute the test error for model 3
lda_fit_model3$error_rate
#model 1
test_error_model1_list = c()
for (i in 1:100) {
#construct a test data
train_set_model1 = sample(c(TRUE,FALSE),nrow(BreastCancer_data_rebuild),replace=TRUE)
lr_train_model1 = glm(y~.,data = BreastCancer_data_rebuild[train_set_model1,], family="binomial")
phat_test1 = predict(lr_train_model1,BreastCancer_data_rebuild[!train_set_model1,],family="response")
yhat_test1 = ifelse(phat_test1>0.5,1,0)
#phat_test1
#compute the test error for model 1
test_error_model1 = 1 - mean(BreastCancer_data_rebuild$y[!train_set_model1] == yhat_test1)
test_error_model1_list = c(test_error_model1_list,test_error_model1)
}
#model 1
#construct a test data
train_set_model1 = sample(c(TRUE,FALSE),nrow(BreastCancer_data_rebuild),replace=TRUE)
lr_train_model1 = glm(y~.,data = BreastCancer_data_rebuild[train_set_model1,], family="binomial")
phat_test1 = predict(lr_train_model1,BreastCancer_data_rebuild[!train_set_model1,],family="response")
yhat_test1 = ifelse(phat_test1>0.5,1,0)
#phat_test1
#compute the test error for model 1
test_error_model1 = 1 - mean(BreastCancer_data_rebuild$y[!train_set_model1] == yhat_test1)
test_error_model1
#model 2
#construct a test data
train_set_model2 = sample(c(TRUE,FALSE),nrow(BreastCancer_data),replace=TRUE)
lr_train_model2 = glmnet(BreastCancer_data[train_set_model2,-1], BreastCancer_data[train_set_model2,1], alpha=0, family="binomial", lambda=lambda_ridge_min )
testdata = apply(BreastCancer_data[!train_set_model2,-1],2,as.numeric)
phat_test2 = predict(lr_train_model2,testdata,s=lambda_ridge_min,family="response")
yhat_test2 = ifelse(phat_test2>0.5,1,0)
#compute the test error for model 2
test_error_model2 = 1 - mean(BreastCancer_data$y[!train_set_model2] == yhat_test2)
test_error_model2
#model 3
library(nclSLR)
training_indices = which(train_set_model2)
validation_indices = which(!train_set_model2)
lda_fit_model3 = linDA(variables = BreastCancer_data[,-1],group = BreastCancer_data$y,validation = "learntest",
learn =training_indices,test=validation_indices )
#compute the test error for model 3
lda_fit_model3$error_rate
#model 1
for(i in 1:10){
#construct a test data
train_set_model1 = sample(c(TRUE,FALSE),nrow(BreastCancer_data_rebuild),replace=TRUE)
lr_train_model1 = glm(y~.,data = BreastCancer_data_rebuild[train_set_model1,], family="binomial")
phat_test1 = predict(lr_train_model1,BreastCancer_data_rebuild[!train_set_model1,],family="response")
yhat_test1 = ifelse(phat_test1>0.5,1,0)
#phat_test1
#compute the test error for model 1
test_error_model1 = 1 - mean(BreastCancer_data_rebuild$y[!train_set_model1] == yhat_test1)
test_error_model1
}
#model 2
#construct a test data
train_set_model2 = sample(c(TRUE,FALSE),nrow(BreastCancer_data),replace=TRUE)
lr_train_model2 = glmnet(BreastCancer_data[train_set_model2,-1], BreastCancer_data[train_set_model2,1], alpha=0, family="binomial", lambda=lambda_ridge_min )
testdata = apply(BreastCancer_data[!train_set_model2,-1],2,as.numeric)
phat_test2 = predict(lr_train_model2,testdata,s=lambda_ridge_min,family="response")
yhat_test2 = ifelse(phat_test2>0.5,1,0)
#compute the test error for model 2
test_error_model2 = 1 - mean(BreastCancer_data$y[!train_set_model2] == yhat_test2)
test_error_model2
#model 3
library(nclSLR)
training_indices = which(train_set_model2)
validation_indices = which(!train_set_model2)
lda_fit_model3 = linDA(variables = BreastCancer_data[,-1],group = BreastCancer_data$y,validation = "learntest",
learn =training_indices,test=validation_indices )
#compute the test error for model 3
lda_fit_model3$error_rate
#model 1
j=0
for(i in 1:10){
#construct a test data
train_set_model1 = sample(c(TRUE,FALSE),nrow(BreastCancer_data_rebuild),replace=TRUE)
lr_train_model1 = glm(y~.,data = BreastCancer_data_rebuild[train_set_model1,], family="binomial")
phat_test1 = predict(lr_train_model1,BreastCancer_data_rebuild[!train_set_model1,],family="response")
yhat_test1 = ifelse(phat_test1>0.5,1,0)
#phat_test1
#compute the test error for model 1
test_error_model1 = 1 - mean(BreastCancer_data_rebuild$y[!train_set_model1] == yhat_test1)
j=j+1
}
j
#model 2
#construct a test data
train_set_model2 = sample(c(TRUE,FALSE),nrow(BreastCancer_data),replace=TRUE)
lr_train_model2 = glmnet(BreastCancer_data[train_set_model2,-1], BreastCancer_data[train_set_model2,1], alpha=0, family="binomial", lambda=lambda_ridge_min )
testdata = apply(BreastCancer_data[!train_set_model2,-1],2,as.numeric)
phat_test2 = predict(lr_train_model2,testdata,s=lambda_ridge_min,family="response")
yhat_test2 = ifelse(phat_test2>0.5,1,0)
#compute the test error for model 2
test_error_model2 = 1 - mean(BreastCancer_data$y[!train_set_model2] == yhat_test2)
test_error_model2
#model 3
library(nclSLR)
training_indices = which(train_set_model2)
validation_indices = which(!train_set_model2)
lda_fit_model3 = linDA(variables = BreastCancer_data[,-1],group = BreastCancer_data$y,validation = "learntest",
learn =training_indices,test=validation_indices )
#compute the test error for model 3
lda_fit_model3$error_rate
#Compare the performance of your models using cross-validation based on the test error
model1_list = c()
#model 1
#construct a test data
for (i in 1:10) {
train_set_model1 = sample(c(TRUE,FALSE),nrow(BreastCancer_data_rebuild),replace=TRUE)
lr_train_model1 = glm(y~.,data = BreastCancer_data_rebuild[train_set_model1,], family="binomial")
phat_test1 = predict(lr_train_model1,BreastCancer_data_rebuild[!train_set_model1,],family="response")
yhat_test1 = ifelse(phat_test1>0.5,1,0)
phat_test1
#compute the test error for model 1
test_error_model1 = 1 - mean(BreastCancer_data_rebuild$y[!train_set_model1] == yhat_test1)
model1_list = c(model1_list,test_error_model1)
}
mean_model1 = mean(model1_list)
mean_model1
#model 1
model1_list = c()
#construct a test data
for (i in 1:100) {
train_set_model1 = sample(c(TRUE,FALSE),nrow(BreastCancer_data_rebuild),replace=TRUE)
lr_train_model1 = glm(y~.,data = BreastCancer_data_rebuild[train_set_model1,], family="binomial")
phat_test1 = predict(lr_train_model1,BreastCancer_data_rebuild[!train_set_model1,],family="response")
yhat_test1 = ifelse(phat_test1>0.5,1,0)
phat_test1
#compute the test error for model 1
test_error_model1 = 1 - mean(BreastCancer_data_rebuild$y[!train_set_model1] == yhat_test1)
model1_list = c(model1_list,test_error_model1)
}
mean_model1 = mean(model1_list)
mean_model1
#model 2
model2_list = c()
model3_list = c()
for(i in 1:100){
#construct a test data
train_set_model2 = sample(c(TRUE,FALSE),nrow(BreastCancer_data),replace=TRUE)
lr_train_model2 = glmnet(BreastCancer_data[train_set_model2,-1], BreastCancer_data[train_set_model2,1], alpha=0, family="binomial", lambda=lambda_ridge_min )
testdata = apply(BreastCancer_data[!train_set_model2,-1],2,as.numeric)
phat_test2 = predict(lr_train_model2,testdata,s=lambda_ridge_min,family="response")
yhat_test2 = ifelse(phat_test2>0.5,1,0)
#compute the test error for model 2
test_error_model2 = 1 - mean(BreastCancer_data$y[!train_set_model2] == yhat_test2)
model2_list = c(model2_list,test_error_model2)
#model 3
library(nclSLR)
training_indices = which(train_set_model2)
validation_indices = which(!train_set_model2)
lda_fit_model3 = linDA(variables = BreastCancer_data[,-1],group = BreastCancer_data$y,validation = "learntest",
learn =training_indices,test=validation_indices )
#compute the test error for model 3
lda_fit_model3$error_rate
model3_list = c(model3_list,lda_fit_model3$error_rate)
}
mean_model2 = mean(model2_list)
mean_model2
mean_model3 = mean(model3_list)
mean_model3
#Compare the performance of your models using cross-validation based on the test error
model1_list = c()
#model 1
#construct a test data
for (i in 1:100) {
train_set_model1 = sample(c(TRUE,FALSE),nrow(BreastCancer_data_rebuild),replace=TRUE)
lr_train_model1 = glm(y~.,data = BreastCancer_data_rebuild[train_set_model1,], family="binomial")
phat_test1 = predict(lr_train_model1,BreastCancer_data_rebuild[!train_set_model1,],family="response")
yhat_test1 = ifelse(phat_test1>0.5,1,0)
phat_test1
#compute the test error for model 1
test_error_model1 = 1 - mean(BreastCancer_data_rebuild$y[!train_set_model1] == yhat_test1)
model1_list = c(model1_list,test_error_model1)
}
mean_model1 = mean(model1_list)
mean_model1
tinytex::tlmgr_update()
options(tinytex.verbose = TRUE)
q()
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir= normalizePath('..'))
library(ProjectTemplate)
load.project()
tinytex::parse_install()
tinytex::parse_install()
update.packages(ask = FALSE, checkBuilt = TRUE)
tinytex::tlmgr_update()
options(tinytex.verbose = TRUE)
options(tinytex.install_packages = TRUE)
library("ProjectTemplate")
setwd("E:/NewcastleStudy/CSC8631/CSC8631Project")
create.project("CY")
setwd("CY/reports")
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir= normalizePath('..'))
tinytex::install_tinytex()
library(tinytex)
tinytex:::install_prebuilt(pkg = "D:/Tools/TinyTeX.zip")
tinytex::tinytex_root()
tinytex:::install_prebuilt(pkg = "D:/TinyTeX.zip",dir = "E:/Tools/TinyTex")
tinytex::uninstall_tinytex()
library(tinytex)
tinytex:::install_prebuilt(pkg = "D:/Tools/TinyTeX.zip")
tl_pkgs()
tinytex::tlmgr_install("texlive-msg-translations")
knitr::opts_chunk$set(echo = TRUE)
library(ProjectTemplate)
load.project()
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir= normalizePath(".."))
